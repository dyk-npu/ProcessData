import os
import pathlib
import signal
import tempfile
import traceback  # å¯¼å…¥æ–°ä¾èµ–ï¼Œç”¨äºè·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
from itertools import repeat
from multiprocessing import Manager
from multiprocessing.pool import Pool

import dgl
import numpy as np
import torch
import pyvista as pv
from OCC.Core.BRepBuilderAPI import BRepBuilderAPI_Sewing
from OCC.Core.StlAPI import StlAPI_Reader
from OCC.Core.TopoDS import TopoDS_Shape
from occwl.graph import face_adjacency
from occwl.shell import Shell
from occwl.uvgrid import ugrid
from tqdm import tqdm

# --- 1. ç”¨æˆ·é…ç½®åŒº ---
INPUT_BASE_DIR = r"E:\CADæ•°æ®é›†\MCB\MCB_A"
OUTPUT_FOLDER_NAME = "bin"
CURV_U_SAMPLES = 10
SURF_U_SAMPLES = 10
SURF_V_SAMPLES = 10
NUM_PROCESSES = 12
SKIP_EXISTING = True
MAX_FILE_SIZE_KB = 150000

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (å·²åŠ å›º) ---

def load_and_convert_mesh(file_path):
    tmp_path = None
    try:
        mesh = pv.read(file_path)
        if mesh.n_points == 0: return None
        with tempfile.NamedTemporaryFile(suffix='.stl', delete=False) as tmpfile:
            tmp_path = tmpfile.name
        mesh.save(tmp_path, binary=True)
        reader = StlAPI_Reader()
        shape = TopoDS_Shape()
        if not reader.Read(shape, tmp_path): return None
        sewing = BRepBuilderAPI_Sewing(1e-6)
        sewing.Add(shape)
        sewing.Perform()
        sewn_shape = sewing.SewedShape()
        return Shell(sewn_shape) if sewn_shape and not sewn_shape.IsNull() else Shell(shape)
    finally:
        if tmp_path and os.path.exists(tmp_path): os.remove(tmp_path)

def build_graph(solid, config):
    """ã€å·²åŠ å›ºã€‘æ ¹æ®ç»™å®šçš„å®ä½“æ„å»ºDGLå›¾ï¼Œå¢åŠ äº†å¤šé‡é˜²å¾¡æ€§æ£€æŸ¥ã€‚"""
    graph = face_adjacency(solid)
    # æ£€æŸ¥1: ç¡®ä¿ä»å®ä½“æˆåŠŸç”Ÿæˆäº†å›¾ï¼Œå¹¶ä¸”å›¾ä¸­æœ‰èŠ‚ç‚¹ï¼ˆé¢ï¼‰
    if not graph or not graph.nodes:
        raise ValueError("Failed to create a valid face-adjacency graph from the solid.")
    
    graph_face_feat, graph_edge_feat = [], []
    for face_idx in graph.nodes:
        face = graph.nodes[face_idx]["face"]
        points = ugrid(face, "point", config['surf_u'], config['surf_v'])
        normals = ugrid(face, "normal", config['surf_u'], config['surf_v'])
        vis = ugrid(face, "visibility_status", config['surf_u'], config['surf_v'])
        mask = np.logical_or(vis == 0, vis == 2)
        graph_face_feat.append(np.concatenate((points, normals, mask), axis=-1))
    
    # æ£€æŸ¥2: ç¡®ä¿æˆåŠŸæå–äº†è‡³å°‘ä¸€ä¸ªé¢çš„ç‰¹å¾
    if not graph_face_feat:
        raise ValueError("Could not extract any face features from the graph.")

    # åªæœ‰å›¾ä¸­æœ‰è¾¹æ—¶æ‰å¤„ç†è¾¹ç‰¹å¾
    if graph.edges:
        for edge_idx in graph.edges:
            edge = graph.edges[edge_idx]["edge"]
            if not edge.has_curve(): continue
            points = ugrid(edge, "point", config['curv_u'])
            tangents = ugrid(edge, "tangent", config['curv_u'])
            graph_edge_feat.append(np.concatenate((points, tangents), axis=-1))
        
        # æ£€æŸ¥3: å¦‚æœæœ‰è¾¹ï¼Œè¦ç¡®ä¿è‡³å°‘æå–äº†ä¸€ä¸ªè¾¹çš„ç‰¹å¾
        if not graph_edge_feat:
            raise ValueError("Graph has edges, but failed to extract any edge features.")

    src, dst = zip(*graph.edges) if graph.edges else ([], [])
    dgl_graph = dgl.graph((src, dst), num_nodes=len(graph.nodes))
    dgl_graph.ndata["x"] = torch.from_numpy(np.asarray(graph_face_feat)).float()
    
    if graph_edge_feat:
        dgl_graph.edata["x"] = torch.from_numpy(np.asarray(graph_edge_feat)).float()
    return dgl_graph

def process_one_file(args_tuple):
    """ã€å·²å‡çº§ã€‘å¤„ç†å•ä¸ªæ–‡ä»¶çš„workerå‡½æ•°ï¼Œç°åœ¨èƒ½è®°å½•è¯¦ç»†çš„é”™è¯¯å †æ ˆã€‚"""
    file_path, output_dir, config, stats = args_tuple
    output_file = output_dir / (file_path.stem + ".bin")

    try:
        if config['skip_existing'] and output_file.exists():
            with stats['lock']: stats['skipped_exist'] += 1
            return
        file_size_kb = file_path.stat().st_size / 1024
        if file_size_kb > config['max_size']:
            with stats['lock']: stats['skipped_size'] += 1
            return
        solid = load_and_convert_mesh(file_path)
        if solid is None: raise ValueError("Failed to load or convert mesh.")
        graph = build_graph(solid, config)
        dgl.save_graphs(str(output_file), [graph])
        with stats['lock']: stats['processed'] += 1
    except Exception:
        # --- è¿™æ˜¯å…³é”®çš„å‡çº§ ---
        # æ•è·ä»»ä½•å¼‚å¸¸ï¼Œå¹¶è®°å½•å®Œæ•´çš„é”™è¯¯å †æ ˆä¿¡æ¯
        with stats['lock']:
            stats['failed'] += 1
            failed_log = stats['failed_log']
            # ä½¿ç”¨ traceback.format_exc() è·å–å®Œæ•´çš„é”™è¯¯æŠ¥å‘Š
            error_details = traceback.format_exc()
            failed_log.append(f"File: {file_path}\n--- Traceback ---\n{error_details}\n-----------------\n")
            stats['failed_log'] = failed_log

def initializer():
    signal.signal(signal.SIGINT, signal.SIG_IGN)

def run_batch_processing():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    train_input_dir = pathlib.Path(INPUT_BASE_DIR) / 'train'
    bin_output_dir = pathlib.Path(INPUT_BASE_DIR) / OUTPUT_FOLDER_NAME / 'train'
    log_file_path = pathlib.Path(INPUT_BASE_DIR) / OUTPUT_FOLDER_NAME / "error_log.txt"

    if not train_input_dir.is_dir():
        print(f"é”™è¯¯ï¼šè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {train_input_dir}")
        return

    bin_output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å…¥ç›®å½•: {train_input_dir}")
    print(f"è¾“å‡ºç›®å½•: {bin_output_dir}")
    print(f"è¯¦ç»†é”™è¯¯æ—¥å¿—å°†ä¿å­˜åœ¨: {log_file_path}")

    print("\næ­£åœ¨æ‰«ææ–‡ä»¶...")
    files_to_process = list(train_input_dir.rglob("*.obj")) + list(train_input_dir.rglob("*.stl"))
    if not files_to_process:
        print("æœªæ‰¾åˆ°ä»»ä½• .obj æˆ– .stl æ–‡ä»¶ã€‚")
        return
    print(f"æ‰¾åˆ° {len(files_to_process)} ä¸ªå¾…å¤„ç†æ–‡ä»¶ã€‚")

    manager = Manager()
    stats = manager.dict({
        'processed': 0, 'skipped_exist': 0, 'skipped_size': 0, 'failed': 0,
        'lock': manager.Lock(), 'failed_log': manager.list()
    })
    config = {
        'curv_u': CURV_U_SAMPLES, 'surf_u': SURF_U_SAMPLES, 'surf_v': SURF_V_SAMPLES,
        'skip_existing': SKIP_EXISTING, 'max_size': MAX_FILE_SIZE_KB
    }
    tasks = []
    for f_path in files_to_process:
        relative_path = f_path.relative_to(train_input_dir)
        output_subdir = bin_output_dir / relative_path.parent
        output_subdir.mkdir(parents=True, exist_ok=True)
        tasks.append((f_path, output_subdir, config, stats))

    print(f"å³å°†ä½¿ç”¨ {NUM_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç†...")
    pool = Pool(NUM_PROCESSES, initializer)
    try:
        list(tqdm(pool.imap_unordered(process_one_file, tasks), total=len(tasks), desc="Processing files"))
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­äº†ç¨‹åºã€‚æ­£åœ¨ç»ˆæ­¢...")
    finally:
        pool.terminate()
        pool.join()

    print("\n" + "="*60)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"âœ… æˆåŠŸå¤„ç†: {stats['processed']} ä¸ªæ–‡ä»¶")
    print(f"â­ï¸  è·³è¿‡ (å·²å­˜åœ¨): {stats['skipped_exist']} ä¸ªæ–‡ä»¶")
    print(f"â­ï¸  è·³è¿‡ (æ–‡ä»¶å¤ªå¤§): {stats['skipped_size']} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤„ç†å¤±è´¥: {stats['failed']} ä¸ªæ–‡ä»¶")

    # å°†è¯¦ç»†çš„å¤±è´¥æ—¥å¿—å†™å…¥æ–‡ä»¶
    if stats['failed'] > 0:
        print(f"\næ£€æµ‹åˆ° {stats['failed']} ä¸ªå¤„ç†å¤±è´¥çš„æ–‡ä»¶ï¼Œè¯¦ç»†ä¿¡æ¯å·²å†™å…¥æ—¥å¿—ã€‚")
        with open(log_file_path, "w", encoding="utf-8") as f:
            f.write(f"æ€»è®¡å¤±è´¥æ–‡ä»¶æ•°: {stats['failed']}\n\n")
            for entry in stats['failed_log']:
                f.write(entry)

if __name__ == "__main__":
    run_batch_processing()